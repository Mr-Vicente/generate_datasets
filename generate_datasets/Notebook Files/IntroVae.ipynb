{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IntroVaeLast_v2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWIlPsHjOlLL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "    152x152 IntroVAE\n",
        "    \n",
        "    Frederico Vicente, NOVA FCT, MIEI\n",
        "    Ludwig Krippahl\n",
        "\"\"\"\n",
        "import data_access\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense,Reshape,Conv2D,Flatten,UpSampling2D,LeakyReLU,Cropping2D,AveragePooling2D,BatchNormalization,ZeroPadding2D,ReLU,InputLayer,Activation,Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.initializers import RandomNormal\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "class ResidualBlock(tf.keras.Model):\n",
        "    def __init__(self, input_channels, out_channels, stride=(1,1), activation=None):\n",
        "        super().__init__(name = \"Residual_Block\")\n",
        "        if(input_channels != out_channels):\n",
        "            self.conv_expand = Conv2D(out_channels, (1,1), padding='same', use_bias=False)\n",
        "        else:\n",
        "            self.conv_expand = None\n",
        "        self.activation = activation\n",
        "        self.conv_1 = Conv2D(out_channels,(3,3), padding='same',use_bias=False)\n",
        "        self.batch_norm_1 = BatchNormalization()\n",
        "        self.leaky_1 = LeakyReLU(alpha=0.2)\n",
        "        self.conv_2 = Conv2D(out_channels,(3,3), padding='same',use_bias=False)\n",
        "        self.batch_norm_2 = BatchNormalization()\n",
        "        self.leaky_2 = LeakyReLU(alpha=0.2)\n",
        "        \n",
        "    def call(self, input_tensor, training = True):\n",
        "        if(self.conv_expand is not None):\n",
        "            residual = self.conv_expand(input_tensor)\n",
        "        else:\n",
        "            residual = input_tensor\n",
        "        out = self.conv_1(input_tensor)\n",
        "        out = self.batch_norm_1(out, training = training)\n",
        "        out = self.leaky_1(out)\n",
        "        out = self.conv_2(out)\n",
        "        out += residual\n",
        "        out = self.batch_norm_2(out, training = training)\n",
        "        \n",
        "        if self.activation is None:\n",
        "            out = self.leaky_2(out)\n",
        "        else:\n",
        "            out = Activation(self.activation)(out)\n",
        "        \n",
        "        return out\n",
        "    \n",
        "class Generator(tf.keras.Model):\n",
        "    def __init__(self, latent_dim = 256, batch_size = 64, channels=[32,64,64,128,128]):\n",
        "        super().__init__(name = \"Generator\")\n",
        "        cc = channels[-1]\n",
        "        self.batch_size = batch_size\n",
        "        self.latent_dim = latent_dim\n",
        "        self.dense_1 = Dense( 8* batch_size * 5 * 5, name = 'Generator_Dense_1', input_shape=(self.latent_dim,))\n",
        "        self.relu = ReLU()\n",
        "        self.reshape_1 = Reshape((5,5, 8 * batch_size))\n",
        "        self.reses = list()\n",
        "        for ch in reversed(channels[:-1]):\n",
        "            print(cc)\n",
        "            self.reses.append([ResidualBlock(cc,ch),UpSampling2D()])\n",
        "            cc = ch\n",
        "        self.res_block_n = ResidualBlock(cc,cc)\n",
        "        self.toRGB = Conv2D(3, (5,5),activation = 'tanh',padding='same', use_bias=False,name = 'Generator_To_RGB')\n",
        "        self.optimizer = Adam(learning_rate=0.0002,beta_1=0.5,beta_2=0.9)\n",
        "        \n",
        "    def call(self, input_tensor, training = True):\n",
        "        x = self.dense_1(input_tensor)\n",
        "        x = self.relu(x)\n",
        "        x = self.reshape_1(x)\n",
        "        for i in range(len(self.reses)):\n",
        "            x = self.reses[i][0](x, training = training)\n",
        "            x = self.reses[i][1](x, training = training)\n",
        "            if(i == 1):\n",
        "               x = Cropping2D(cropping=((1,0),(1,0)))(x)\n",
        "        x = self.res_block_n(x, training = training)\n",
        "        x = self.toRGB(x)\n",
        "        return x\n",
        "\n",
        "    def backPropagate(self,gradients,trainable_variables):\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "\n",
        "    def set_seed(self):\n",
        "        self.seed = tf.random.normal([self.batch_size, self.latent_dim])\n",
        "        data_access.store_seed_in_file('seed',self.seed)\n",
        "\n",
        "    def load_seed(self):\n",
        "        self.seed = data_access.load_seed_from_file('seed')\n",
        "      \n",
        "    \n",
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, latent_dim = 256, channels=[32,64,64,128,128]):\n",
        "        super().__init__(name = \"Encoder\")\n",
        "        self.conv_1 = Conv2D(16, (5,5), padding='same', use_bias=False, input_shape = (152,152,3))\n",
        "        self.batch_norm_1 = BatchNormalization()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.leaky_1 = LeakyReLU(alpha=0.2)\n",
        "        self.downsampling = AveragePooling2D()\n",
        "        self.reses = list()\n",
        "        cc = channels[0]\n",
        "        for ch in channels[1:]:\n",
        "            print(cc)\n",
        "            self.reses.append([ResidualBlock(cc,ch),AveragePooling2D()])\n",
        "            cc = ch\n",
        "        self.res_block_n = ResidualBlock(cc,cc)#,activation = 'linear')\n",
        "        self.flat = Flatten()\n",
        "        self.dense = Dense(self.latent_dim * 2)\n",
        "        self.optimizer = Adam(learning_rate=0.0002,beta_1=0.5,beta_2=0.9)\n",
        "        \n",
        "    def call(self, input_tensor, training = True):\n",
        "        out = self.conv_1(input_tensor)\n",
        "        out = self.batch_norm_1(out, training = training)\n",
        "        out = self.leaky_1(out)\n",
        "        out = self.downsampling(out)\n",
        "        for i in range(len(self.reses)):\n",
        "            out = self.reses[i][0](out, training = training)\n",
        "            out = self.reses[i][1](out, training = training)\n",
        "        out = self.res_block_n(out, training = training)\n",
        "        out = self.flat(out)\n",
        "        out = self.dense(out)\n",
        "        return out \n",
        "\n",
        "    def backPropagate(self,gradients,trainable_variables):\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "       \n",
        "class IntroVae(tf.keras.Model):\n",
        "    def __init__(self, params = None, channels=[16,32,64,128,256,512]):\n",
        "        super().__init__(name = \"Intro_Vae\")\n",
        "\n",
        "        if params is None: \n",
        "            params = {\n",
        "              'batchsz': 64,\n",
        "              'z_dim': 256,\n",
        "              'epochs': 200,\n",
        "              'm': 30,\n",
        "              'alpha': 0.25,\n",
        "              'beta': 1.0,\n",
        "              'gamma': 1.0,\n",
        "              'lr': 0.0001\n",
        "            }\n",
        "        self.params = params\n",
        "        \n",
        "        self.latent_dim = params['z_dim']\n",
        "        self.set_learning_constants(m = params['m'], alpha = params['alpha'], beta = params['beta'])\n",
        "        self.inference_net = Encoder(latent_dim = self.latent_dim, channels = channels)\n",
        "        self.generative_net = Generator(latent_dim = self.latent_dim, channels = channels)\n",
        "        if('seed.npz' not in os.listdir('.')):\n",
        "            self.generative_net.set_seed()\n",
        "        else :\n",
        "            self.generative_net.load_seed()\n",
        "        \n",
        "        self.train_dataset = None\n",
        "    \n",
        "    def set_learning_constants(self, m=110, alpha=0.25, beta=0.5):\n",
        "        self.m = m\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        \n",
        "    def load_dataset(self,dataset):\n",
        "        \"\"\"\n",
        "        Load images as numpy vectors and store dataset number of classes\n",
        "        \"\"\"\n",
        "        self.train_dataset,_,_,_ = dataset\n",
        "        print('Dataset loaded')\n",
        "        \n",
        "    \n",
        "    def sample(self, eps=None):\n",
        "        if eps is None:\n",
        "          eps = tf.random.normal(shape=(self.params['batchsz'], self.latent_dim))\n",
        "        return self.decode(eps, train = False)\n",
        "\n",
        "    @tf.function   \n",
        "    def encode(self, x, train = True):\n",
        "        mean, logvar = tf.split(self.inference_net(x, training = train), num_or_size_splits=2, axis=1)\n",
        "        return mean, logvar\n",
        "\n",
        "    @tf.function\n",
        "    def reparameterize(self, mean, logvar):\n",
        "        eps = tf.random.normal(shape=mean.shape)\n",
        "        return eps * tf.exp(logvar * .5) + mean\n",
        "\n",
        "    @tf.function\n",
        "    def decode(self, z, train = True):\n",
        "        logits = self.generative_net(z, training = train)\n",
        "        return logits\n",
        "\n",
        "    @tf.function\n",
        "    def kl_loss(self,mean, logvar):\n",
        "        kl_div = tf.reduce_mean(-0.5 * tf.reduce_sum(1 + logvar - tf.square(mean)- tf.exp(logvar), axis=-1))\n",
        "        return kl_div\n",
        "      \n",
        "      \n",
        "    @tf.function\n",
        "    def L_AE(self,gen,real):\n",
        "        mse = tf.reduce_mean(tf.square(tf.subtract(real,gen)))\n",
        "        return mse\n",
        "\n",
        "    @tf.function    \n",
        "    def compute_loss_inf(self, z, mean, logv , z_r, mean_r, logv_r, z_pp, mean_pp, logv_pp, gen, real):\n",
        "        regr_ng = tf.math.maximum(0.0, self.m - self.kl_loss(mean_r, logv_r)) \n",
        "        regpp_ng = tf.math.maximum(0.0, self.m - self.kl_loss(mean_pp, logv_pp)) \n",
        "        regs_ng = self.alpha * (regr_ng + regpp_ng)\n",
        "        reg_ae = self.kl_loss(mean, logv)\n",
        "        #tf.print('regr_ng: ', self.kl_loss(mean_r, logv_r))\n",
        "        #tf.print('regpp_ng: ', self.kl_loss(mean_pp, logv_pp))\n",
        "        #tf.print('reg_ae: ',reg_ae)\n",
        "        #tf.print('MSE_inf: ',self.beta * self.L_AE(gen,real))\n",
        "        return reg_ae + regs_ng + (self.beta * self.L_AE(gen,real))\n",
        "\n",
        "    @tf.function\n",
        "    def compute_loss_gen(self, z_r, mean_r, logv_r, z_pp, mean_pp, logv_pp, gen, real):\n",
        "        regr = self.kl_loss(mean_r, logv_r)\n",
        "        regpp = self.kl_loss(mean_pp, logv_pp)\n",
        "        regs_adv = self.alpha * (regr + regpp)\n",
        "        tf.print('regs_adv: ',regs_adv)\n",
        "        tf.print('MSE_gen: ',self.beta * self.L_AE(gen,real))\n",
        "        return regs_adv + (self.beta * self.L_AE(gen,real))\n",
        "\n",
        "    @tf.function\n",
        "    def compute_apply_gradients(self,x):\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            #tf.print('--------start-------')\n",
        "            mean, logvar = self.encode(x, train = True)\n",
        "            z = self.reparameterize(mean, logvar)\n",
        "            x_r = self.decode(z, train = True)\n",
        "            z_prior = tf.random.normal(shape=mean.shape)\n",
        "            x_p = self.decode(z_prior, train = True)\n",
        "            #tf.print('mean: ',tf.reduce_mean(mean))\n",
        "            #tf.print('logvar: ',tf.reduce_mean(logvar))\n",
        "            #tf.print('x_logit: ',tf.reduce_mean(x_r))\n",
        "            #tf.print('z_prior: ',tf.reduce_mean(z_prior))\n",
        "            #tf.print('x_p: ',tf.reduce_mean(x_p))\n",
        "\n",
        "            #mean_ng, logvar_ng = self.encode(x, train = False)\n",
        "            #z_ng = self.reparameterize(mean_ng, logvar_ng)\n",
        "            #x_r_ng = self.decode(z, train = False)\n",
        "            #x_p_ng = self.decode(z_prior, train = False)\n",
        "\n",
        "            #tf.print('-------inf--------')\n",
        "            mean_r, logvar_r = self.encode(tf.stop_gradient(x_r)) # ng(.) no training\n",
        "            z_r =  self.reparameterize(mean_r, logvar_r)\n",
        "\n",
        "\n",
        "            #tf.print('mean_r: ',tf.reduce_mean(mean_r))\n",
        "            #tf.print('logvar_r: ',tf.reduce_mean(logvar_r))\n",
        "\n",
        "            mean_pp, logvar_pp = self.encode(tf.stop_gradient(x_p)) # ng(.) no training\n",
        "            z_pp =  self.reparameterize(mean_pp, logvar_pp)\n",
        "\n",
        "            #tf.print('mean_pp: ',tf.reduce_mean(mean_pp))\n",
        "            #tf.print('logvar_pp: ',tf.reduce_mean(logvar_pp))\n",
        "\n",
        "            inf_loss = self.compute_loss_inf(z, mean, logvar, z_r, mean_r , logvar_r , z_pp, mean_pp, logvar_pp, x_r, x)\n",
        "            #tf.print('--------gen-------')\n",
        "            mean_r, logvar_r = self.encode(x_r, train = True)\n",
        "            z_r =  self.reparameterize(mean_r, logvar_r) \n",
        "            mean_pp, logvar_pp = self.encode(x_p, train = True)\n",
        "            z_pp =  self.reparameterize(mean_pp, logvar_pp)\n",
        "            \n",
        "            gen_loss = self.compute_loss_gen(z_r, mean_r,logvar_r, z_pp, mean_pp, logvar_pp, x_r, x)\n",
        "\n",
        "        gradients_of_inf = tape.gradient(inf_loss, self.inference_net.trainable_variables)\n",
        "        gradients_of_gen = tape.gradient(gen_loss, self.generative_net.trainable_variables)\n",
        "        self.inference_net.backPropagate(gradients_of_inf, self.inference_net.trainable_variables)\n",
        "        self.generative_net.backPropagate(gradients_of_gen, self.generative_net.trainable_variables)\n",
        "        return inf_loss, gen_loss\n",
        "\n",
        "    def generate_real_samples(self, n_samples):\n",
        "        # choose random instances\n",
        "        ix = np.random.randint(0, self.train_dataset.shape[0], n_samples)\n",
        "        # select images\n",
        "        X = self.train_dataset[ix]\n",
        "        #convert X to tensor\n",
        "        return tf.convert_to_tensor(X.astype(np.float32))\n",
        "        \n",
        "    def train_model(self,epochs = None ,batch_size = None, images_per_epoch=4,directory='imgs'):\n",
        "        if epochs is None: epochs = self.params['epochs']\n",
        "        if batch_size is None: batch_size = self.params['batchsz']\n",
        "\n",
        "        batch_per_epoch = int(self.train_dataset.shape[0] / batch_size)\n",
        "        # calculate the number of training iterations\n",
        "        n_steps = batch_per_epoch * epochs\n",
        "\n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            epoch = int(open('current_epoch.txt').read())\n",
        "        except:\n",
        "            epoch = 0\n",
        "\n",
        "        for step_i in range(n_steps):\n",
        "            train_x = self.generate_real_samples(batch_size)\n",
        "            inf_loss, gen_loss = self.compute_apply_gradients(train_x)\n",
        "            data_access.print_training_output_vae(step_i,n_steps,inf_loss,gen_loss)\n",
        "            if((step_i % (n_steps / epochs)) == 0):\n",
        "                epoch += 1\n",
        "                gen_images = self.sample(self.generative_net.seed)\n",
        "                data_access.store_images_seed(directory,gen_images[:images_per_epoch],epoch)\n",
        "                self.generative_net.save_weights('weights/g_weights/g_weights',save_format='tf')\n",
        "                self.inference_net.save_weights('weights/i_weights/i_weights',save_format='tf')\n",
        "                data_access.write_current_epoch(filename='current_epoch',epoch=epoch)\n",
        "        end_time = time.time()\n",
        "        data_access.print_training_time(start_time,end_time,self.params)\n",
        "\n",
        "    def generate_images(self,number_of_samples=5,directory=\"imgs\"):\n",
        "        random_vector_for_generation = tf.random.normal(shape=[number_of_samples, self.latent_dim])\n",
        "        predictions = self.sample(random_vector_for_generation)\n",
        "        data_access.prepare_directory(directory)\n",
        "        data_access.store_images_seed(directory,predictions,'None','gan')\n",
        "    \n",
        "    def generate_image(self, z):\n",
        "        return self.sample(z)\n",
        "\n",
        "    def get_latent_code(self, image):\n",
        "        mean, logvar = self.encode(image,training=False)\n",
        "        l_code = self.reparameterize(mean, logvar)\n",
        "        return l_code.numpy()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ixODqsFayM3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 20\n",
        "OUT_DIR = \"gen_imgs\"\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "vae = IntroVae()\n",
        "vae.load_dataset(data_access.prepare_data('gan'))\n",
        "\n",
        "vae.train_model()\n",
        "vae.generate_images(5,OUT_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzs4nh5AUdJA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    from google.colab import files\n",
        "except ImportError:\n",
        "    pass\n",
        "else:\n",
        "    !zip -r /content/weights_2.zip /content/weights\n",
        "    files.download(\"/content/weights_2.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LH9xMRdbUeIy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r imgs\n",
        "!rm -r current_epoch.txt\n",
        "!rm -r logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ab_XR_gfB9gI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}