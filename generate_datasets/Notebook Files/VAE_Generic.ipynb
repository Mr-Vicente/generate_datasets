{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE_Generic.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8TuLEac-JzT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89KLU9YCWTA-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "    VAE\n",
        "    \n",
        "    Frederico Vicente, NOVA FCT, MIEI\n",
        "    Ludwig Krippahl\n",
        "\"\"\"\n",
        "import data_access\n",
        "import process_cartoon\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import InputLayer,Dense,Conv2DTranspose,Reshape,Conv2D,Flatten,LeakyReLU,UpSampling2D,Cropping2D,ReLU\n",
        "from tensorflow.keras.initializers import RandomNormal\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from IPython import display\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "class Inference_net(tf.keras.Model):\n",
        "    def __init__(self,latent_dim=128):\n",
        "        super().__init__(name='inference')\n",
        "         #layers\n",
        "        self.conv_1 = Conv2D(filters=32, kernel_size=3, strides=(2, 2), activation='relu',input_shape=(128,128,3))\n",
        "        self.conv_2 = Conv2D(filters=64, kernel_size=3, strides=(2, 2), activation='relu')\n",
        "        self.conv_3 = Conv2D(filters=128, kernel_size=3, strides=(2, 2), activation='relu')\n",
        "        self.conv_4 = Conv2D(filters=256, kernel_size=3, strides=(2, 2), activation='relu')\n",
        "        self.conv_5 = Conv2D(filters=512, kernel_size=3, strides=(2, 2), activation='relu')\n",
        "        self.flat = Flatten()\n",
        "        self.last_inf = Dense(latent_dim + latent_dim)\n",
        "        \n",
        "    def call(self, input_tensor):\n",
        "        ## Definition of Forward Pass\n",
        "        x = self.conv_1(input_tensor)\n",
        "        x = self.conv_2(x)\n",
        "        x = self.conv_3(x)\n",
        "        x = self.conv_4(x)\n",
        "        x = self.conv_5(x)\n",
        "        x = self.last_inf(self.flat(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class Generator_net(tf.keras.Model):\n",
        "    def __init__(self,batch_size = 64, latent_dim=128):\n",
        "        super().__init__(name='Generator')\n",
        "         #layers\n",
        "        self.batch_size = batch_size\n",
        "        self.latent_dim = latent_dim\n",
        "        self.dense_1 = Dense(units=8 * batch_size * 4*4, activation=\"relu\",input_shape=(latent_dim,))\n",
        "        self.reshape = Reshape(target_shape=(4, 4, batch_size*8))\n",
        "\n",
        "        self.up_1 = UpSampling2D((2,2), interpolation='nearest')\n",
        "        self.conv_1 = Conv2D(filters=8 * batch_size, kernel_size=(3, 3), strides = (1,1), padding = \"same\")\n",
        "        self.leaky_1 = ReLU()\n",
        "\n",
        "        self.up_2 = UpSampling2D((2,2), interpolation='nearest')\n",
        "        self.conv_2 = Conv2D(filters=4 * batch_size, kernel_size=(3, 3), strides = (1,1), padding = \"same\")\n",
        "        self.leaky_2 = ReLU()\n",
        "\n",
        "        self.up_3 = UpSampling2D((2,2), interpolation='nearest')\n",
        "        self.conv_3 = Conv2D(filters=2 * batch_size, kernel_size=(3, 3), strides = (1,1), padding = \"same\")\n",
        "        self.leaky_3 = ReLU()\n",
        "\n",
        "        self.up_4 = UpSampling2D((2,2), interpolation='nearest')\n",
        "        self.conv_4 = Conv2D(filters=batch_size, kernel_size=(3, 3), strides = (1,1), padding = \"same\")\n",
        "        self.leaky_4 = ReLU()\n",
        "\n",
        "        self.up_5 = UpSampling2D((2,2), interpolation='nearest')\n",
        "        self.conv_5 = Conv2D(filters=batch_size//2, kernel_size=(3, 3), strides = (1,1), padding = \"same\")\n",
        "        self.leaky_5 = ReLU()\n",
        "\n",
        "        self.conv_6 = Conv2D(filters=3, kernel_size=(3, 3), strides = (1,1), padding = \"same\")\n",
        "\n",
        "    def call(self, input_tensor):\n",
        "        ## Definition of Forward Pass\n",
        "        x = self.dense_1(input_tensor)\n",
        "        x = self.reshape(x)\n",
        "        x = self.leaky_1(self.conv_1(self.up_1(x)))\n",
        "        x = self.leaky_2(self.conv_2(self.up_2(x)))\n",
        "        x = self.leaky_3(self.conv_3(self.up_3(x)))\n",
        "        x = self.leaky_4(self.conv_4(self.up_4(x)))\n",
        "        x = self.leaky_5(self.conv_5(self.up_5(x)))\n",
        "        x = self.conv_6(x)\n",
        "        return x\n",
        "\n",
        "    def set_seed(self):\n",
        "        self.seed = tf.random.normal([self.batch_size, self.latent_dim])\n",
        "        data_access.store_seed_in_file('seed',self.seed)\n",
        "\n",
        "    def load_seed(self):\n",
        "        self.seed = data_access.load_seed_from_file('seed')\n",
        "\n",
        "class VAE(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self,batch_size = 64, latent_dim = 128):\n",
        "        super().__init__(name='vae')\n",
        "        self.latent_dim = latent_dim\n",
        "        self.batch_size = batch_size\n",
        "        self.inference_net = Inference_net(self.latent_dim)\n",
        "        self.generative_net = Generator_net(batch_size, self.latent_dim)\n",
        "        self.optimizer = Adam(1e-4)\n",
        "\n",
        "        if('seed.npz' not in os.listdir('.')):\n",
        "            self.generative_net.set_seed()\n",
        "        else :\n",
        "            self.generative_net.load_seed()\n",
        "        \n",
        "        self.train_dataset = None\n",
        "        self.test_dataset = None\n",
        "        self.train_labels = None\n",
        "        self.test_labels = None\n",
        "        \n",
        "    def load_dataset(self,dataset):\n",
        "        self.train_dataset,self.train_labels,self.test_dataset,self.test_labels = dataset\n",
        "        self.train_dataset = data_access.normalize(self.train_dataset)\n",
        "\n",
        "    @tf.function\n",
        "    def sample(self, eps=None,training = True):\n",
        "        if eps is None:\n",
        "          eps = tf.random.normal(shape=(100, self.latent_dim))\n",
        "        return self.decode(eps, apply_sigmoid=True,training=training)\n",
        "        \n",
        "    @tf.function\n",
        "    def encode(self, x, training=True):\n",
        "        mean, logvar = tf.split(self.inference_net(x,training=training), num_or_size_splits=2, axis=1)\n",
        "        return mean, logvar\n",
        "\n",
        "    def reparameterize(self, mean, logvar):\n",
        "        eps = tf.random.normal(shape=mean.shape)\n",
        "        return eps * tf.exp(logvar * .5) + mean\n",
        "\n",
        "    @tf.function\n",
        "    def decode(self, z, apply_sigmoid=False,training = True):\n",
        "        logits = self.generative_net(z,training=training)\n",
        "        if apply_sigmoid:\n",
        "          probs = tf.sigmoid(logits)\n",
        "          return probs\n",
        "        return logits\n",
        "    \n",
        "    def log_normal_pdf(self,sample, mean, logvar, raxis=1):\n",
        "        log2pi = tf.math.log(2. * np.pi)\n",
        "        return tf.reduce_sum(\n",
        "            -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
        "            axis=raxis)\n",
        "        \n",
        "    @tf.function\n",
        "    def compute_loss(self, x):\n",
        "        mean, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mean, logvar)\n",
        "        x_logit = self.decode(z)\n",
        "\n",
        "        cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)\n",
        "        logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n",
        "        logpz = self.log_normal_pdf(z, 0., 0.)\n",
        "        logqz_x = self.log_normal_pdf(z, mean, logvar)\n",
        "        return -tf.reduce_mean(logpx_z + logpz - logqz_x)\n",
        "\n",
        "    @tf.function\n",
        "    def compute_apply_gradients(self,x):\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = self.compute_loss(x)\n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "        return loss\n",
        "\n",
        "    def define_loss_tensorboard(self):\n",
        "        logdir=\"logs/train/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "        return tf.summary.create_file_writer(logdir=logdir)\n",
        "\n",
        "    def generate_real_samples(self, n_samples):\n",
        "        # choose random instances\n",
        "        ix = np.random.randint(0, self.train_dataset.shape[0], n_samples)\n",
        "        # select images\n",
        "        X = self.train_dataset[ix]\n",
        "        #convert X to tensor\n",
        "        return tf.convert_to_tensor(X.astype(np.float32))\n",
        "\n",
        "\n",
        "    def train_model(self,epochs,batch_size=None,directory='imgs',images_per_epoch=4):\n",
        "        if batch_size is None: batch_size = self.batch_size\n",
        "        sum_writer_loss = self.define_loss_tensorboard()\n",
        "        avg_vae_loss = tf.keras.metrics.Mean()\n",
        "        start_time = time.time()\n",
        "\n",
        "        batch_per_epoch = int(self.train_dataset.shape[0] / batch_size)\n",
        "        # calculate the number of training iterations\n",
        "        n_steps = batch_per_epoch * epochs\n",
        "\n",
        "        try:\n",
        "            epoch = int(open('current_epoch.txt').read())\n",
        "        except:\n",
        "            epoch = 0\n",
        "\n",
        "        for step_i in range(n_steps):\n",
        "            train_x = self.generate_real_samples(batch_size)\n",
        "            loss = self.compute_apply_gradients(train_x)\n",
        "            avg_vae_loss(loss)\n",
        "            if((step_i % (n_steps / epochs)) == 0):\n",
        "                epoch += 1\n",
        "                gen_images = self.sample(self.generative_net.seed,False)\n",
        "                data_access.store_images_seed(directory,gen_images[:images_per_epoch],epoch,'vae')\n",
        "                data_access.write_current_epoch('current_epoch',epoch=epoch)\n",
        "            data_access.print_training_output_simple_loss(step_i,n_steps,loss)\n",
        "            with sum_writer_loss.as_default():\n",
        "                tf.summary.scalar('loss_vae', avg_vae_loss.result(),step=self.optimizer.iterations)\n",
        "        end_time = time.time()\n",
        "        data_access.print_training_time(start_time,end_time,None)\n",
        "\n",
        "    def generate_images(self,number_of_samples=5,directory=\"imgs\"):\n",
        "        random_vector_for_generation = tf.random.normal(shape=[number_of_samples, self.latent_dim])\n",
        "        images = self.sample(random_vector_for_generation,False)\n",
        "        data_access.store_images_seed(directory,images,'None','vae')\n",
        "\n",
        "    def get_latent_code(self, image):\n",
        "        mean, logvar = self.encode(image,training=False)\n",
        "        l_code = self.reparameterize(mean, logvar)\n",
        "        return l_code.numpy()\n",
        "    \n",
        "    def interpolate_c1_to_c2(self, images=None, labels=None , c1=0, c2=1 , alphas=np.linspace(-2, 2, 20)):\n",
        "        if images is None: images = self.train_dataset\n",
        "        if labels is None: labels = self.train_labels\n",
        "        n_samples = alphas.shape[0]\n",
        "        figsize=(14 * int(n_samples / 10),20)\n",
        "        #Find centroids of class c1 and c2\n",
        "        z_c1_avg = self.get_latent_code(images[labels == c1]).mean(axis=0)\n",
        "        z_c2_avg = self.get_latent_code(images[labels == c2]).mean(axis=0)\n",
        "\n",
        "        #Find medoid of class c1\n",
        "        z_c1_med = np.median(self.get_latent_code(images[labels == c1]), axis=0)\n",
        "\n",
        "        #Interpolation vector c1->c2\n",
        "        z_c1_c2 = z_c2_avg - z_c1_avg\n",
        "\n",
        "        x_gens = []\n",
        "        for alpha in alphas:\n",
        "            z_interp = z_c1_med + alpha * z_c1_c2\n",
        "            z_interp = z_interp.reshape(1,-1)\n",
        "            image = self.sample(z_interp, training=False)\n",
        "            x_gens.append(image)\n",
        "        self.display_images(x_gens,figsize)\n",
        "        return x_gens\n",
        "\n",
        "    def interpolate_many_c1_to_c2(self, images=None, labels=None , c1=0, c2=1 , alphas=np.linspace(-2, 2, 20), class_names=None):\n",
        "        if images is None: images = self.train_dataset\n",
        "        if labels is None: labels = self.train_labels\n",
        "        n_samples = alphas.shape[0]\n",
        "        figsize=(14 * int(n_samples / 10),20)\n",
        "        #Find latent code of classes\n",
        "        mask_c1 = np.argmax(labels,axis=1) == c1 \n",
        "        mask_c2 = np.argmax(labels,axis=1) == c2 \n",
        "        z_c1 = self.get_latent_code(images[mask_c1])[:400]\n",
        "        z_c2 = self.get_latent_code(images[mask_c2])[:400]\n",
        "\n",
        "        N_SPLITS = 2\n",
        "        c1_splits = np.array(np.split(z_c1, N_SPLITS)).astype(np.float32)\n",
        "        c2_splits = np.array(np.split(z_c2, N_SPLITS)).astype(np.float32)\n",
        "\n",
        "        classifier = tf.keras.models.load_model('hair_classifier_v09933.h5')\n",
        "        for i_c in range(N_SPLITS):\n",
        "            z_c1_i = c1_splits[i_c]\n",
        "            z_c2_i = c2_splits[i_c]\n",
        "            z_c1_i_avg = z_c1_i.mean(axis=0)\n",
        "            z_c2_i_avg = z_c2_i.mean(axis=0)\n",
        "\n",
        "            c_diff = z_c2_i_avg - z_c1_i_avg\n",
        "\n",
        "            #Find medoid of class c1\n",
        "            z_c1_i_med = np.median(z_c1_i, axis=0)\n",
        "\n",
        "            x_gens = []\n",
        "            for alpha in alphas:\n",
        "                z_interp = z_c1_i_med + alpha * c_diff\n",
        "                z_interp = z_interp.reshape(1,-1)\n",
        "                image = self.sample(z_interp, training=False)\n",
        "                x_gens.append(image)\n",
        "            self.display_images(x_gens,figsize)\n",
        "            self.plot_alpha_probs(x_gens, classifier, classes = [c1,c2], class_names=class_names)\n",
        "        return x_gens\n",
        "          \n",
        "    def interpolate_c1_to_c2_though_c3(self, images=None, labels=None , c1=0, c2=1, c3=2, alphas=np.linspace(-2, 2, 20)):\n",
        "        if images is None: images = self.train_dataset\n",
        "        if labels is None: labels = self.train_labels\n",
        "        n_samples = alphas.shape[0]\n",
        "        figsize=(14 * int(n_samples / 10),20)\n",
        "        #Find centroids of class c1 and c2\n",
        "        z_c1_avg = self.get_latent_code(images[labels == c1]).mean(axis=0)\n",
        "        z_c2_avg = self.get_latent_code(images[labels == c2]).mean(axis=0)\n",
        "\n",
        "        #Find medoid of class c1\n",
        "        z_c3_med = np.median(self.get_latent_code(images[labels == c3]), axis=0)\n",
        "\n",
        "        #Interpolation vector c1->c2\n",
        "        z_c1_c2 = z_c2_avg - z_c1_avg\n",
        "\n",
        "        x_gens = []\n",
        "        for alpha in alphas:\n",
        "            z_interp = z_c3_med + alpha * z_c1_c2\n",
        "            z_interp = z_interp.reshape(1,-1)\n",
        "            image = self.sample(z_interp, training=False)\n",
        "            x_gens.append(image)\n",
        "        self.display_images(x_gens,figsize)\n",
        "        return x_gens  \n",
        "    \n",
        "    def display_images(self, images, figsize=None):\n",
        "        n_images = len(images)\n",
        "        plt.subplots(figsize=figsize, squeeze=False)\n",
        "        for i, image in enumerate(images):\n",
        "            plt.subplot(1, n_images, i+1)\n",
        "            if (image.shape[-1] == 1):\n",
        "                plt.imshow(image[0,:,:,0], cmap='gray')\n",
        "            else:\n",
        "                plt.imshow(image[0,:,:,:])\n",
        "            plt.axis('off')\n",
        "\n",
        "    def plot_alpha_probs(self,images, classifier, alphas=np.linspace(-2, 2, 20), classes=[0,1],class_names=None):\n",
        "\n",
        "        fig, ax = plt.subplots()\n",
        "        for c in classes:\n",
        "            cs = []\n",
        "            for image in images:\n",
        "                prediction = classifier(image).numpy()[0]\n",
        "                t_class = prediction[c]\n",
        "                cs.append(t_class)\n",
        "            ax.plot(alphas,cs, label=class_names[c])\n",
        "\n",
        "        ax.legend(loc='best')\n",
        "        ax.set_ylabel('Probability')\n",
        "        ax.set_xlabel('delta distance')\n",
        "        "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9pHxRocci2l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 200\n",
        "latent_dim = 128\n",
        "n_images = 100\n",
        "OUTPUT_DIR = \"imgs_gen\"\n",
        "\n",
        "class_names = ['Blond-yellow','Yellow','Orange','Orange-Brown','Blond','Light-Brown','Brown','Black','Gray','White']\n",
        "\n",
        "def process_cartoon_data():\n",
        "    images, labels = process_cartoon.decode_data_cartoon()\n",
        "    return images,labels, None, None\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "model = VAE(latent_dim)\n",
        "model.load_dataset(process_cartoon_data())\n",
        "model.train_model(epochs)\n",
        "model.generate_images(n_images,OUTPUT_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTSf3wYRahSt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images = model.interpolate_many_c1_to_c2(c1=7,c2=9,class_names=class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLd58y-4qthP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r imgs\n",
        "!rm -r logs\n",
        "!rm -r imgs_gen\n",
        "!rm -r current_epoch.txt"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wJsJ9j_bJdY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}