{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WGAN_GENERIC_CARTOON.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTU_Ne2qVvpK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEX74gMYVy3B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "    Cartoon WGAN GP WITH CLASSIFIER\n",
        "    \n",
        "    Frederico Vicente, NOVA FCT, MIEI\n",
        "    Ludwig Krippahl\n",
        "\"\"\"\n",
        "\n",
        "import data_access\n",
        "import process_cartoon\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense,BatchNormalization,Reshape,Conv2D,Dropout,Flatten,UpSampling2D,LeakyReLU,Cropping2D,LayerNormalization,ReLU\n",
        "from tensorflow.keras.initializers import RandomNormal\n",
        "from tensorflow.keras.optimizers import RMSprop,Adam\n",
        "from tensorflow.keras.losses import categorical_crossentropy,KLDivergence\n",
        "from tensorflow.keras import backend\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "from IPython import display\n",
        "from datetime import datetime\n",
        "import os\n",
        "import math\n",
        "\n",
        "class Generator(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self, model_parameters=None):\n",
        "        super().__init__(name='generator')\n",
        "        #layers\n",
        "        if model_parameters is None:\n",
        "            model_parameters = {\n",
        "                'lr': 0.0001,\n",
        "                'beta1': 0,\n",
        "                'batch_size': 64,\n",
        "                'latent_dim': 128,\n",
        "                'image_size': 152\n",
        "            }\n",
        "        self.model_parameters = model_parameters\n",
        "        self.batch_size = model_parameters['batch_size']\n",
        "        self.noise_size = model_parameters['latent_dim']\n",
        "        dim = 8 * self.batch_size\n",
        "        init = RandomNormal(stddev=0.02)\n",
        "        self.dense_1 = Dense(dim*4*4, use_bias = False, input_shape = (self.noise_size,))\n",
        "        self.batchNorm1 = BatchNormalization()\n",
        "        self.leaky_1 = ReLU()\n",
        "        self.reshape_1 = Reshape((4,4,dim))\n",
        "        self.layers_blocks = list()\n",
        "        \n",
        "        number_of_layers_needed = int(math.log(model_parameters['image_size'],2))-3\n",
        "        for i in range(number_of_layers_needed):\n",
        "            dim /= 2\n",
        "            self.layers_blocks.append([\n",
        "                UpSampling2D((2,2), interpolation='nearest'),\n",
        "                Conv2D(dim, (5, 5), strides = (1,1), padding = \"same\", use_bias = False, kernel_initializer=init),\n",
        "                BatchNormalization(),\n",
        "                ReLU(),\n",
        "            ])\n",
        "    \n",
        "        self.up_toRGB = UpSampling2D((2,2), interpolation='nearest')\n",
        "        self.conv_toRGB = Conv2D(3, (5, 5), activation='tanh', strides = (1,1), padding = \"same\", use_bias = False, kernel_initializer=init)\n",
        "        \n",
        "        self.optimizer = Adam(learning_rate=model_parameters['lr'],beta_1=model_parameters['beta1'],beta_2=0.9)\n",
        "\n",
        "    def call(self, input_tensor, training = True):\n",
        "        ## Definition of Forward Pass\n",
        "        x = self.leaky_1(self.batchNorm1(self.reshape_1(self.dense_1(input_tensor)),training = training))\n",
        "        for i in range(len(self.layers_blocks)):\n",
        "            layers_block = self.layers_blocks[i]\n",
        "            for layer in layers_block:\n",
        "                x = layer(x, training = training)\n",
        "        x = self.conv_toRGB(self.up_toRGB(x))\n",
        "        return x\n",
        "    \n",
        "    def generate_noise(self,batch_size, random_noise_size):\n",
        "        return tf.random.normal([batch_size, random_noise_size])\n",
        "\n",
        "    def compute_loss(self,y_true,y_pred):\n",
        "        return backend.mean(y_true * y_pred)\n",
        "\n",
        "    def compute_loss_class(self,y_true,y_pred,class_wanted,class_prediction):\n",
        "        \"\"\" Wasserstein loss - prob of classifier get it right\n",
        "        \"\"\"\n",
        "        k = 10 # hiper-parameter\n",
        "        return backend.mean(y_true * y_pred) + (k * categorical_crossentropy(class_wanted,class_prediction))\n",
        "\n",
        "    def compute_loss_divergence(self,y_true,y_pred,class_wanted,class_prediction):\n",
        "        k = 10 # hiper-parameter\n",
        "        kl = KLDivergence()\n",
        "        return backend.mean(y_true * y_pred) + (k * kl(class_wanted,class_prediction))\n",
        "\n",
        "    def backPropagate(self,gradients,trainable_variables):\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "\n",
        "    def save_optimizer(self):\n",
        "        weights = self.optimizer.get_weights()\n",
        "        data_access.store_weights_in_file('g_optimizer_weights',weights)\n",
        "\n",
        "    def set_seed(self):\n",
        "        self.seed = tf.random.normal([self.batch_size, self.noise_size])\n",
        "        data_access.store_seed_in_file('seed',self.seed)\n",
        "\n",
        "    def load_seed(self):\n",
        "        self.seed = data_access.load_seed_from_file('seed')\n",
        "      \n",
        "class Critic(tf.keras.Model):\n",
        "    def __init__(self,model_parameters=None):\n",
        "        super().__init__(name = \"critic\")\n",
        "        if model_parameters is None:\n",
        "            model_parameters = {\n",
        "                'lr': 0.0001,\n",
        "                'beta1': 0,\n",
        "                'batch_size': 64,\n",
        "                'latent_dim': 128,\n",
        "                'image_size': 152\n",
        "            }\n",
        "        self.layers_blocks = list()\n",
        "        self.model_parameters = model_parameters\n",
        "        dim = model_parameters['batch_size'] / 2\n",
        "        init = RandomNormal(stddev=0.02)\n",
        "        #Layers\n",
        "        self.conv_1 = Conv2D(dim, (5, 5), strides=(2, 2), padding='same', kernel_initializer=init, input_shape=[model_parameters['image_size'], model_parameters['image_size'], 3])\n",
        "        self.leaky_1 = LeakyReLU(alpha=0.2)\n",
        "        \n",
        "        number_of_layers_needed = int(math.log(model_parameters['image_size'],2))-3\n",
        "        for i in range(number_of_layers_needed):\n",
        "            dim *= 2\n",
        "            self.layers_blocks.append([\n",
        "               Conv2D(dim, (5, 5), strides=(2, 2), padding='same', kernel_initializer=init),\n",
        "               LayerNormalization(),\n",
        "               LeakyReLU(alpha=0.2)\n",
        "            ])\n",
        "\n",
        "        self.flat = Flatten()\n",
        "        self.logits = Dense(1)  # This neuron tells us how real or fake the input is\n",
        "        \n",
        "        self.optimizer = Adam(learning_rate=model_parameters['lr'],beta_1=model_parameters['beta1'],beta_2=0.9)\n",
        "\n",
        "    def call(self, input_tensor, training = True):\n",
        "        ## Definition of Forward Pass\n",
        "        x = self.leaky_1(self.conv_1(input_tensor))\n",
        "        for i in range(len(self.layers_blocks)):\n",
        "            layers_block = self.layers_blocks[i]\n",
        "            for layer in layers_block:\n",
        "                x = layer(x, training = training)\n",
        "        x = self.flat(x)\n",
        "        return self.logits(x)\n",
        "\n",
        "    def compute_loss(self,y_true,y_pred):\n",
        "        \"\"\" Wasserstein loss\n",
        "        \"\"\"\n",
        "        return backend.mean(y_true * y_pred) \n",
        "\n",
        "    def backPropagate(self,gradients,trainable_variables):\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "\n",
        "    def save_optimizer(self):\n",
        "        weights = self.optimizer.get_weights()\n",
        "        data_access.store_weights_in_file('c_optimizer_weights',weights)\n",
        "        \n",
        "        \n",
        "        \n",
        "class Big_WGAN(tf.keras.Model):\n",
        "    def __init__(self, model_parameters = None, classifier_filename=None, classifier_study_type = \"class_convergence\"):\n",
        "        super().__init__(name = \"BIG_WGAN\")\n",
        "\n",
        "        if model_parameters is None:\n",
        "            model_parameters = {\n",
        "                'lr': 0.0001,\n",
        "                'beta1': 0,\n",
        "                'batch_size': 64,\n",
        "                'latent_dim': 128,\n",
        "                'image_size': 152\n",
        "            }\n",
        "        # only accept power of 2 sizes\n",
        "        model_parameters['image_size'] = 2**int(math.log(model_parameters['image_size'],2))\n",
        "        self.model_parameters = model_parameters\n",
        "        self.random_noise_size = model_parameters['latent_dim']\n",
        "        self.generator = Generator(model_parameters)\n",
        "        if('seed.npz' not in os.listdir('.')):\n",
        "            self.generator.set_seed()\n",
        "        else :\n",
        "            self.generator.load_seed()\n",
        "        self.critic = Critic(model_parameters)\n",
        "        if ('weights' in os.listdir('.')):\n",
        "            self.critic.load_weights('/content/weights/c_weights/c_weights')\n",
        "            self.generator.load_weights('/content/weights/g_weights/g_weights')\n",
        "        self.classifier = None\n",
        "        if classifier_filename is not None:\n",
        "            self.classifier = tf.keras.models.load_model(classifier_filename)\n",
        "        self.classifier_study_type = classifier_study_type\n",
        "               \n",
        "        self.train_dataset = None\n",
        "        self.test_dataset = None\n",
        "        self.train_labels = None\n",
        "        self.test_labels = None\n",
        "        self.batch_size = model_parameters['batch_size']\n",
        "        \n",
        "    def load_dataset(self,dataset,n_classes):\n",
        "        self.train_dataset,self.train_labels,self.test_dataset,self.test_labels = dataset\n",
        "        self.train_dataset = data_access.standardize(self.train_dataset)\n",
        "        self.num_classes = n_classes\n",
        "\n",
        "    @tf.function\n",
        "    def predict_batch(self,images,type_class):\n",
        "        images_predictions = tf.TensorArray(tf.float32,size=0,dynamic_size=True)\n",
        "        ys = tf.TensorArray(tf.float32,size=0,dynamic_size=True)\n",
        "        matched_images = tf.TensorArray(tf.float32,size=0,dynamic_size=True)\n",
        "        index = 0\n",
        "        basis = tf.convert_to_tensor([0,1],dtype=tf.float32)\n",
        "        for i in tf.range(len(images)):\n",
        "            gen_image = data_access.normalize(data_access.de_standardize(images[i]))\n",
        "            img = tf.expand_dims(gen_image,axis=0)\n",
        "            c = self.classifier(img)\n",
        "            if(self.num_classes == 2):\n",
        "                x = tf.subtract(c,basis)\n",
        "                w_list = tf.abs(x)\n",
        "            else:\n",
        "                w_list = c\n",
        "            w_list = tf.reshape(w_list,(w_list.shape[1],))\n",
        "\n",
        "            images_predictions = images_predictions.write(i,w_list)\n",
        "\n",
        "            if self.classifier_study_type == 'class_convergence':\n",
        "                y_list = tf.one_hot(type_class,self.num_classes)\n",
        "            elif self.classifier_study_type == 'class_divergence':\n",
        "                y_list = tf.convert_to_tensor(type_class,dtype=tf.float32)\n",
        "\n",
        "            ys = ys.write(i,y_list)\n",
        "            if(tf.reduce_all(tf.equal(w_list,y_list))):\n",
        "                matched_images = matched_images.write(index,images[i])\n",
        "                index +=1\n",
        "                \n",
        "        return images_predictions.stack(), ys.stack(),matched_images.stack()\n",
        "\n",
        "    @tf.function\n",
        "    def gradient_penalty(self,generated_samples,real_images,half_batch):\n",
        "        alpha = backend.random_uniform(shape=[half_batch,1,1,1],minval=0.0,maxval=1.0)\n",
        "        differences = generated_samples - real_images\n",
        "        interpolates = real_images + (alpha * differences)\n",
        "        gradients = tf.gradients(self.critic(interpolates),[interpolates])[0]\n",
        "        slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients),axis=[1,2,3]))\n",
        "        gradient_p = tf.reduce_mean((slopes-1.)**2)\n",
        "        return gradient_p\n",
        "\n",
        "    @tf.function\n",
        "    def training_step_critic(self,real_imgs,gen_imgs,real_labels,gen_labels,half_batch):\n",
        "        lambda_ = 10.0\n",
        "        with tf.GradientTape() as tape:\n",
        "            d_x_real = self.critic(real_imgs, training=True) \n",
        "            d_x_gen = self.critic(gen_imgs, training=True) \n",
        "            critic_r_loss = self.critic.compute_loss(real_labels, d_x_real)\n",
        "            critic_g_loss = self.critic.compute_loss(gen_labels, d_x_gen)\n",
        "            total_loss = critic_r_loss + critic_g_loss + (lambda_ * self.gradient_penalty(gen_imgs,real_imgs,half_batch))\n",
        "        \n",
        "        gradients_of_critic = tape.gradient(total_loss, self.critic.trainable_variables)\n",
        "        self.critic.backPropagate(gradients_of_critic, self.critic.trainable_variables)\n",
        "        return total_loss\n",
        "\n",
        "\n",
        "    @tf.function\n",
        "    def training_step_generator(self,noise_size,class_type):\n",
        "        # prepare points in latent space as input for the generator\n",
        "        X_g = self.generator.generate_noise(self.batch_size,noise_size)\n",
        "        # create inverted labels for the fake samples\n",
        "        y_g = -np.ones((self.batch_size, 1)).astype(np.float32)\n",
        "        with tf.GradientTape() as tape:\n",
        "            d_x = self.generator(X_g, training=True) # Trainable?\n",
        "            d_z = self.critic(d_x, training=True)\n",
        "            if self.classifier is not None:\n",
        "                images_predictions, ys, matched_images = self.predict_batch(d_x,class_type)\n",
        "                if self.classifier_study_type == 'class_convergence':\n",
        "                    generator_loss = self.generator.compute_loss_class(d_z, y_g, ys, images_predictions)\n",
        "                elif self.classifier_study_type == 'class_divergence':\n",
        "                    generator_loss = self.generator.compute_loss_divergence(d_z, y_g, ys, images_predictions)\n",
        "            else:\n",
        "                images_predictions, ys, matched_images = [],[],[]\n",
        "                generator_loss = self.generator.compute_loss(d_z, y_g)\n",
        "        \n",
        "        gradients_of_generator = tape.gradient(generator_loss, self.generator.trainable_variables)\n",
        "        self.generator.backPropagate(gradients_of_generator, self.generator.trainable_variables)\n",
        "        return generator_loss,matched_images, self.generator(self.generator.seed, training=False)\n",
        "\n",
        "    def generate_real_samples(self, n_samples):\n",
        "        # choose random instances\n",
        "        ix = np.random.randint(0, self.train_dataset.shape[0], n_samples)\n",
        "        # select images\n",
        "        X = self.train_dataset[ix]\n",
        "        # associate with class labels of -1 for 'real'\n",
        "        y = -np.ones((n_samples, 1)).astype(np.float32)\n",
        "        return tf.convert_to_tensor(X), y\n",
        "\n",
        "    @tf.function\n",
        "    # use the generator to generate n fake examples, with class labels\n",
        "    def generate_fake_samples(self, noise_size, n_samples):\n",
        "        # generate points in latent space\n",
        "        x_input = self.generator.generate_noise(n_samples,noise_size)\n",
        "        # get images generated\n",
        "        X = self.generator(x_input,training=True)\n",
        "        # associate with class labels of 1.0 for 'fake'\n",
        "        y = np.ones((n_samples, 1)).astype(np.float32)\n",
        "        return X, y\n",
        "\n",
        "    def define_loss_tensorboard(self):\n",
        "        \"\"\"\n",
        "        Tensorboard Integration - loss scallars\n",
        "        \"\"\"\n",
        "        logdir=\"logs/train/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "        return tf.summary.create_file_writer(logdir=logdir)\n",
        "\n",
        "    def define_graph_tensorboard(self):\n",
        "        \"\"\"\n",
        "        Tensorboard Integration - grath\n",
        "        \"\"\"\n",
        "        logdir=\"logs/graph/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "        return tf.summary.create_file_writer(logdir=logdir)\n",
        "    \n",
        "    def train_model(self,epoches,n_critic=5,class_type=9,directory = 'imgs',n_img_per_epoch=4):\n",
        "        \"\"\"\n",
        "        Train model for an amount of epochs\n",
        "\n",
        "        :param epoches: - cycles of training over all dataset\n",
        "        :param n_critic: - number of times critic trains more than generator\n",
        "        :param class_type: - it can be a number if class convergence is wanted\n",
        "            or a probability distribution (array) discribing the probabilities for each class\n",
        "            Example: class convergence(when there are 5 classes): 0\n",
        "                     class divergence(when there are 5 classes): [0.25, 0.25, 0.25, 0.25, 0.0]\n",
        "        :param directory: - directory where images will be placed during training\n",
        "        :n_img_per_epoch: - number of images stored per epoch, while training\n",
        "        \"\"\"\n",
        "        #class_type= [0.0 for i in range(self.num_classes)]\n",
        "        #class_type[7] = 0.5\n",
        "        #class_type[9] = 0.5\n",
        "        batch_per_epoch = int(self.train_dataset.shape[0] / self.batch_size)\n",
        "\n",
        "        # calculate the number of training iterations\n",
        "        n_steps = batch_per_epoch * epoches\n",
        "        # calculate the size of half a batch of samples\n",
        "        half_batch = int(self.batch_size / 2)\n",
        "\n",
        "        sum_writer_loss = self.define_loss_tensorboard()\n",
        "        avg_loss_critic = tf.keras.metrics.Mean()\n",
        "        avg_loss_gen = tf.keras.metrics.Mean()\n",
        "        try:\n",
        "            epoch = int(open('current_epoch.txt').read())\n",
        "        except:\n",
        "            epoch = 0\n",
        "        start_time = time.time()\n",
        "        print(self.model_parameters)\n",
        "        for i in range(n_steps):\n",
        "            for _ in range(n_critic):\n",
        "                # get randomly selected 'real' samples\n",
        "                X_real, y_real = self.generate_real_samples(half_batch)\n",
        "                # generate 'fake' examples\n",
        "                X_fake, y_fake = self.generate_fake_samples(self.random_noise_size, half_batch)\n",
        "                \n",
        "                # update critic model weights\n",
        "                c_loss = self.training_step_critic(X_real,X_fake, y_real,y_fake,half_batch)\n",
        "                avg_loss_critic(c_loss)\n",
        "                \n",
        "            gen_loss, matched_images, gen_images = self.training_step_generator(self.random_noise_size,class_type)\n",
        "            avg_loss_gen(gen_loss)\n",
        "            data_access.print_training_output(i,n_steps, avg_loss_critic.result(),avg_loss_gen.result()) \n",
        "            if((i % (n_steps / epoches)) == 0):\n",
        "                data_access.store_images_seed(directory,gen_images[:n_img_per_epoch],epoch)\n",
        "                with sum_writer_loss.as_default():\n",
        "                    tf.summary.scalar('loss_gen', avg_loss_gen.result(),step=self.generator.optimizer.iterations)\n",
        "                    tf.summary.scalar('avg_loss_critic', avg_loss_critic.result(),step=self.critic.optimizer.iterations)\n",
        "                epoch += 1\n",
        "                if((epoch % 1) == 0):\n",
        "                    self.generator.save_weights('weights/g_weights/g_weights',save_format='tf')\n",
        "                    self.critic.save_weights('weights/c_weights/c_weights',save_format='tf')\n",
        "                    data_access.write_current_epoch(filename='current_epoch',epoch=epoch)\n",
        "        data_access.create_collection(epoches,n_img_per_epoch,directory)\n",
        "        data_access.print_training_time(start_time,time.time(),self.model_parameters)\n",
        "\n",
        "    def generate_images(self,number_of_samples,directory,class_names=None):\n",
        "        seed = tf.random.normal([number_of_samples, self.random_noise_size])\n",
        "        images = self.generator(seed,training=False)\n",
        "        if self.classifier is not None: \n",
        "            predictions = self.classifier(data_access.normalize(data_access.de_standardize(images)))\n",
        "            data_access.produce_generate_figure(directory,images,predictions,class_names)\n",
        "        else:\n",
        "            data_access.store_images_seed(directory,images,'gen_images','gan')\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFQgz686Vyzc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 200\n",
        "\n",
        "class_names = ['Blond-yellow','Yellow','Orange','Orange-Brown','Blond','Light-Brown','Brown','Black','Gray','White']\n",
        "\n",
        "def process_cartoon_data():\n",
        "    images, labels = process_cartoon.decode_data_cartoon()\n",
        "    return images,labels, None, None\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "gan = Big_WGAN(classifier_filename='hair_classifier_v09933_plus.h5')\n",
        "gan.load_dataset(process_cartoon_data(),len(class_names))\n",
        "\n",
        "gan.train_model(EPOCHS)\n",
        "\n",
        "gan.generate_images(100,\"gen_images\", class_names)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoIGspLoXXAF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    from google.colab import files\n",
        "except ImportError:\n",
        "    pass\n",
        "else:\n",
        "    !zip -r /content/gen_images.zip /content/gen_images\n",
        "    files.download(\"/content/gen_images.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeRq9XYiRVZ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r imgs\n",
        "!rm -r weights\n",
        "!rm current_epoch.txt\n",
        "!rm -r logs"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}