{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WGAN_GP_TRAINS.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pa7K2yRaZXrO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_za7de5ZfV5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0003bc7f-2568-4f64-d53d-583d94200d8e"
      },
      "source": [
        "\"\"\"\n",
        "    152x152 WGAN GP WITH CLASSIFIER\n",
        "    \n",
        "    Frederico Vicente, NOVA FCT, MIEI\n",
        "    Ludwig Krippahl\n",
        "\"\"\"\n",
        "\n",
        "import data_access\n",
        "\n",
        "from google.colab import files\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense,BatchNormalization,Reshape,Conv2D,Dropout,Flatten,UpSampling2D,LeakyReLU,Cropping2D,LayerNormalization,ReLU\n",
        "from tensorflow.keras.initializers import RandomNormal\n",
        "from tensorflow.keras.optimizers import RMSprop,Adam\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "from tensorflow.keras import backend\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "from IPython import display\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "class Generator(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self, model_parameters=None):\n",
        "        super().__init__(name='generator')\n",
        "        if model_parameters is None:\n",
        "            model_parameters = {\n",
        "                'lr': 0.0001,\n",
        "                'beta1': 0,\n",
        "                'batch_size': 64,\n",
        "                'latent_dim': 128,\n",
        "                'image_size': 152\n",
        "            }\n",
        "        self.model_parameters = model_parameters\n",
        "\n",
        "        self.batch_size = model_parameters['batch_size']\n",
        "        self.noise_size = model_parameters['latent_dim']\n",
        "        init = RandomNormal(stddev=0.02)\n",
        "        self.dense_1 = Dense(8*5*5*self.batch_size, use_bias = False, input_shape = (self.noise_size,))\n",
        "        self.batchNorm1 = BatchNormalization()\n",
        "        self.leaky_1 = ReLU()\n",
        "        self.reshape_1 = Reshape((5,5,8*self.batch_size))\n",
        "\n",
        "        self.up_2 = UpSampling2D((2,2), interpolation='nearest')\n",
        "        self.conv2 = Conv2D(4*self.batch_size, (5, 5), strides = (1,1), padding = \"same\", use_bias = False, kernel_initializer=init)\n",
        "        self.batchNorm2 = BatchNormalization()\n",
        "        self.leaky_2 = ReLU()\n",
        "        \n",
        "        self.up_3 = UpSampling2D((2,2), interpolation='nearest')\n",
        "        self.conv3 = Conv2D(2*self.batch_size, (5, 5), strides = (1,1), padding = \"same\", use_bias = False, kernel_initializer=init)\n",
        "        self.crop_3 = Cropping2D(cropping=((1,0),(1,0)))\n",
        "        self.batchNorm3 = BatchNormalization()\n",
        "        self.leaky_3 = ReLU()\n",
        "        \n",
        "        self.up_4 = UpSampling2D((2,2), interpolation='nearest')\n",
        "        self.conv4 = Conv2D(self.batch_size, (5, 5), strides = (1,1), padding = \"same\", use_bias = False, kernel_initializer=init)\n",
        "        self.batchNorm4 = BatchNormalization()\n",
        "        self.leaky_4 = ReLU()\n",
        "\n",
        "        self.up_5 = UpSampling2D((2,2), interpolation='nearest')\n",
        "        self.conv5 = Conv2D(self.batch_size/2, (5, 5), strides = (1,1), padding = \"same\", use_bias = False, kernel_initializer=init)\n",
        "        self.batchNorm5 = BatchNormalization()\n",
        "        self.leaky_5 = ReLU()\n",
        "        \n",
        "        self.up_6 = UpSampling2D((2,2), interpolation='nearest')\n",
        "        self.conv6 = Conv2D(3, (5, 5), activation='tanh', strides = (1,1), padding = \"same\", use_bias = False, kernel_initializer=init)\n",
        "\n",
        "        self.optimizer = Adam(learning_rate=model_parameters['lr'],beta_1=model_parameters['beta1'],beta_2=0.9)\n",
        "\n",
        "    def call(self, input_tensor, training = True):\n",
        "        ## Definition of Forward Pass\n",
        "        x = self.leaky_1(self.batchNorm1(self.reshape_1(self.dense_1(input_tensor)),training = training))\n",
        "        x = self.leaky_2(self.batchNorm2(self.conv2(self.up_2(x)),training = training))\n",
        "        x = self.leaky_3(self.batchNorm3(self.crop_3(self.conv3(self.up_3(x))),training = training))\n",
        "        x = self.leaky_4(self.batchNorm4(self.conv4(self.up_4(x)),training = training))\n",
        "        x = self.leaky_5(self.batchNorm5(self.conv5(self.up_5(x)),training = training))\n",
        "        return  self.conv6(self.up_6(x))\n",
        "    \n",
        "    def generate_noise(self,batch_size, random_noise_size):\n",
        "        return tf.random.normal([batch_size, random_noise_size])\n",
        "\n",
        "    def compute_loss(self,y_true,y_pred,class_wanted,class_prediction):\n",
        "        \"\"\" Wasserstein loss - prob of classifier get it right\n",
        "        \"\"\"\n",
        "        k = 10 # hiper-parameter\n",
        "        return backend.mean(y_true * y_pred) + (k * categorical_crossentropy(class_wanted,class_prediction))\n",
        "\n",
        "    def backPropagate(self,gradients,trainable_variables):\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "\n",
        "    def save_optimizer(self):\n",
        "        weights = self.optimizer.get_weights()\n",
        "        data_access.store_weights_in_file('g_optimizer_weights',weights)\n",
        "\n",
        "    def set_seed(self):\n",
        "        self.seed = tf.random.normal([self.batch_size, self.noise_size])\n",
        "        data_access.store_seed_in_file('seed',self.seed)\n",
        "\n",
        "    def load_seed(self):\n",
        "        self.seed = data_access.load_seed_from_file('seed')\n",
        "        \n",
        "class Critic(tf.keras.Model):\n",
        "    def __init__(self,model_parameters=None):\n",
        "        super().__init__(name = \"critic\")\n",
        "        if model_parameters is None:\n",
        "            model_parameters = {\n",
        "                'lr': 0.0001,\n",
        "                'beta1': 0,\n",
        "                'batch_size': 64,\n",
        "                'latent_dim': 128,\n",
        "                'image_size': 152\n",
        "            }\n",
        "        self.model_parameters = model_parameters\n",
        "        init = RandomNormal(stddev=0.02)\n",
        "        #Layers\n",
        "        self.conv_1 = Conv2D(32, (5, 5), strides=(2, 2), padding='same', kernel_initializer=init, input_shape=[model_parameters['image_size'], model_parameters['image_size'], 3])\n",
        "        self.leaky_1 = LeakyReLU(alpha=0.2)\n",
        "        \n",
        "        self.conv_2 = Conv2D(64, (5, 5), strides=(2, 2), padding='same', kernel_initializer=init)\n",
        "        self.layer_norm_2 = LayerNormalization()\n",
        "        self.leaky_2 = LeakyReLU(alpha=0.2)\n",
        "        \n",
        "        self.conv_3 = Conv2D(128, (5, 5), strides=(2, 2), padding='same', kernel_initializer=init)\n",
        "        self.layer_norm_3 = LayerNormalization()\n",
        "        self.leaky_3 = LeakyReLU(alpha=0.2)\n",
        "        \n",
        "        self.conv_4 = Conv2D(256, (5, 5), strides=(2, 2), padding='same', kernel_initializer=init)\n",
        "        self.layer_norm_4 = LayerNormalization()\n",
        "        self.leaky_4 = LeakyReLU(alpha=0.2)\n",
        "\n",
        "        self.conv_5 = Conv2D(512, (5, 5), strides=(2, 2), padding='same', kernel_initializer=init)\n",
        "        self.layer_norm_5 = LayerNormalization()\n",
        "        self.leaky_5 = LeakyReLU(alpha=0.2)\n",
        "\n",
        "        self.flat = Flatten()\n",
        "        self.logits = Dense(1)  # This neuron tells us how real or fake the input is\n",
        "        \n",
        "        self.optimizer = Adam(learning_rate=model_parameters['lr'],beta_1=model_parameters['beta1'],beta_2=0.9)\n",
        "\n",
        "    def call(self, input_tensor, training = True):\n",
        "        ## Definition of Forward Pass\n",
        "\n",
        "        x = self.leaky_1(self.conv_1(input_tensor))\n",
        "        x = self.leaky_2(self.layer_norm_2(self.conv_2(x)))\n",
        "        x = self.leaky_3(self.layer_norm_3(self.conv_3(x)))\n",
        "        x = self.leaky_4(self.layer_norm_4(self.conv_4(x)))\n",
        "        x = self.leaky_5(self.layer_norm_5(self.conv_5(x)))\n",
        "\n",
        "        x = self.flat(x)\n",
        "        return self.logits(x)\n",
        "\n",
        "    def compute_loss(self,y_true,y_pred):\n",
        "        \"\"\" Wasserstein loss\n",
        "        \"\"\"\n",
        "        return backend.mean(y_true * y_pred) \n",
        "\n",
        "    def backPropagate(self,gradients,trainable_variables):\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "\n",
        "    def save_optimizer(self):\n",
        "        weights = self.optimizer.get_weights()\n",
        "        data_access.store_weights_in_file('c_optimizer_weights',weights)\n",
        "        \n",
        "        \n",
        "        \n",
        "class WGAN_GP(tf.keras.Model):\n",
        "    def __init__(self, model_parameters = None, classifier_filename='TypeC_0.99_ConvExp19.hdf5'):\n",
        "        super().__init__(name = \"BIG_WGAN\")\n",
        "        if model_parameters is None:\n",
        "            model_parameters = {\n",
        "                'lr': 0.0001,\n",
        "                'beta1': 0,\n",
        "                'batch_size': 64,\n",
        "                'latent_dim': 128,\n",
        "                'image_size': 152\n",
        "            }\n",
        "        self.model_parameters = model_parameters\n",
        "        self.random_noise_size = model_parameters['latent_dim']\n",
        "        self.generator = Generator(model_parameters)\n",
        "        if('seed.npz' not in os.listdir('.')):\n",
        "            self.generator.set_seed()\n",
        "        else :\n",
        "            self.generator.load_seed()\n",
        "        self.critic = Critic(model_parameters)\n",
        "        if ('weights' in os.listdir('.')):\n",
        "            self.critic.load_weights('/content/weights/c_weights/c_weights')\n",
        "            self.generator.load_weights('/content/weights/g_weights/g_weights')\n",
        "        self.classifier = tf.keras.models.load_model(classifier_filename)\n",
        "               \n",
        "        self.train_dataset = None\n",
        "        self.test_dataset = None\n",
        "        self.train_labels = None\n",
        "        self.test_labels = None\n",
        "        self.batch_size = model_parameters['batch_size']\n",
        "        \n",
        "    def load_dataset(self,dataset,n_classes):\n",
        "        self.train_dataset,self.train_labels,self.test_dataset,self.test_labels = dataset\n",
        "        self.num_classes = n_classes\n",
        "\n",
        "    @tf.function\n",
        "    def predict_batch(self,images,type_class):\n",
        "        images_predictions = tf.TensorArray(tf.float32,size=0,dynamic_size=True)\n",
        "        ys = tf.TensorArray(tf.float32,size=0,dynamic_size=True)\n",
        "        matched_images = tf.TensorArray(tf.float32,size=0,dynamic_size=True)\n",
        "        index = 0\n",
        "        basis = tf.convert_to_tensor([0,1],dtype=tf.float32)\n",
        "        for i in tf.range(len(images)):\n",
        "            gen_image = data_access.normalize(data_access.de_standardize(images[i]))\n",
        "            img = tf.expand_dims(gen_image,axis=0)\n",
        "            c = self.classifier(img)\n",
        "            if(self.num_classes == 2):\n",
        "                x = tf.subtract(c,basis)\n",
        "                w_list = tf.abs(x)\n",
        "            else:\n",
        "                w_list = c\n",
        "            w_list = tf.reshape(w_list,(w_list.shape[1],))\n",
        "            \n",
        "            images_predictions = images_predictions.write(i,w_list)\n",
        "            y_list = tf.one_hot(type_class,self.num_classes)\n",
        "            ys = ys.write(i,y_list)\n",
        "            if(tf.reduce_all(tf.equal(w_list,y_list))):\n",
        "                matched_images = matched_images.write(index,images[i])\n",
        "                index +=1\n",
        "                \n",
        "        return images_predictions.stack(), ys.stack(),matched_images.stack()\n",
        "\n",
        "    @tf.function\n",
        "    def gradient_penalty(self,generated_samples,real_images,half_batch):\n",
        "        alpha = backend.random_uniform(shape=[half_batch,1,1,1],minval=0.0,maxval=1.0)\n",
        "        differences = generated_samples - real_images\n",
        "        interpolates = real_images + (alpha * differences)\n",
        "        gradients = tf.gradients(self.critic(interpolates),[interpolates])[0]\n",
        "        slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients),axis=[1,2,3]))\n",
        "        gradient_p = tf.reduce_mean((slopes-1.)**2)\n",
        "        return gradient_p\n",
        "\n",
        "    @tf.function\n",
        "    def training_step_critic(self,real_imgs,gen_imgs,real_labels,gen_labels,half_batch):\n",
        "        lambda_ = 10.0\n",
        "        with tf.GradientTape() as tape:\n",
        "            d_x_real = self.critic(real_imgs, training=True) \n",
        "            d_x_gen = self.critic(gen_imgs, training=True) \n",
        "            critic_r_loss = self.critic.compute_loss(real_labels, d_x_real)\n",
        "            critic_g_loss = self.critic.compute_loss(gen_labels, d_x_gen)\n",
        "            total_loss = critic_r_loss + critic_g_loss + (lambda_ * self.gradient_penalty(gen_imgs,real_imgs,half_batch))\n",
        "        \n",
        "        gradients_of_critic = tape.gradient(total_loss, self.critic.trainable_variables)\n",
        "        self.critic.backPropagate(gradients_of_critic, self.critic.trainable_variables)\n",
        "        return total_loss\n",
        "\n",
        "\n",
        "    @tf.function\n",
        "    def training_step_generator(self,noise_size,class_type):\n",
        "        # prepare points in latent space as input for the generator\n",
        "        X_g = self.generator.generate_noise(self.batch_size,noise_size)\n",
        "        # create inverted labels for the fake samples\n",
        "        y_g = -np.ones((self.batch_size, 1)).astype(np.float32)\n",
        "        with tf.GradientTape() as tape:\n",
        "            d_x = self.generator(X_g, training=True) # Trainable?\n",
        "            d_z = self.critic(d_x, training=True)\n",
        "            if self.classifier is not None:\n",
        "                images_predictions, ys, matched_images = self.predict_batch(d_x,class_type)\n",
        "                generator_loss = self.generator.compute_loss_class(d_z, y_g, ys, images_predictions)\n",
        "            else:\n",
        "                images_predictions, ys, matched_images = [],[],[]\n",
        "                generator_loss = self.generator.compute_loss(d_z, y_g)\n",
        "                \n",
        "        gradients_of_generator = tape.gradient(generator_loss, self.generator.trainable_variables)\n",
        "        self.generator.backPropagate(gradients_of_generator, self.generator.trainable_variables)\n",
        "        return generator_loss,matched_images, self.generator(self.generator.seed, training=False)\n",
        "\n",
        "    def generate_real_samples(self, n_samples):\n",
        "        # choose random instances\n",
        "        ix = np.random.randint(0, self.train_dataset.shape[0], n_samples)\n",
        "        # select images\n",
        "        X = self.train_dataset[ix]  #tf.gather(self.train_dataset,ix)\n",
        "        # associate with class labels of -1 for 'real'\n",
        "        y = -np.ones((n_samples, 1)).astype(np.float32)\n",
        "        return tf.convert_to_tensor(X), y\n",
        "\n",
        "    @tf.function\n",
        "    # use the generator to generate n fake examples, with class labels\n",
        "    def generate_fake_samples(self, noise_size, n_samples):\n",
        "        # generate points in latent space\n",
        "        x_input = self.generator.generate_noise(n_samples,noise_size)\n",
        "        # get images generated\n",
        "        X = self.generator(x_input,training=True)\n",
        "        # associate with class labels of 1.0 for 'fake'\n",
        "        y = np.ones((n_samples, 1)).astype(np.float32)\n",
        "        return X, y\n",
        "\n",
        "    def define_loss_tensorboard(self):\n",
        "        \"\"\"\n",
        "        Tensorboard Integration - loss scallars\n",
        "        \"\"\"\n",
        "        logdir=\"logs/train/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "        return tf.summary.create_file_writer(logdir=logdir)\n",
        "\n",
        "    def define_graph_tensorboard(self):\n",
        "        \"\"\"\n",
        "        Tensorboard Integration - grath\n",
        "        \"\"\"\n",
        "        logdir=\"logs/graph/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "        return tf.summary.create_file_writer(logdir=logdir)\n",
        "    \n",
        "    def train_model(self,epoches,n_critic=5,class_type=0,directory = 'imgs'):\n",
        "        \"\"\"\n",
        "        Train model for an amount of epochs\n",
        "\n",
        "        :param epoches: - cycles of training over all dataset\n",
        "        :param n_critic: - number of times critic trains more than generator\n",
        "        :param class_type: - class number: converge generated data to this class\n",
        "        :param directory: - directory where images will be placed during training\n",
        "        \"\"\"\n",
        "        batch_per_epoch = int(self.train_dataset.shape[0] / self.batch_size)\n",
        "\n",
        "        # calculate the number of training iterations\n",
        "        n_steps = batch_per_epoch * epoches\n",
        "        # calculate the size of half a batch of samples\n",
        "        half_batch = int(self.batch_size / 2)\n",
        "\n",
        "        sum_writer_loss = self.define_loss_tensorboard()\n",
        "        #self.classifier_m.load_local_model()\n",
        "        avg_loss_critic = tf.keras.metrics.Mean()\n",
        "        avg_loss_gen = tf.keras.metrics.Mean()\n",
        "        try:\n",
        "            epoch = int(open('current_epoch.txt').read())\n",
        "        except:\n",
        "            epoch = 0\n",
        "        n_dif_images = 4\n",
        "        start_time = time.time()\n",
        "        for i in range(n_steps):\n",
        "            for _ in range(n_critic):\n",
        "                # get randomly selected 'real' samples\n",
        "                X_real, y_real = self.generate_real_samples(half_batch)\n",
        "                # generate 'fake' examples\n",
        "                X_fake, y_fake = self.generate_fake_samples(self.random_noise_size, half_batch)\n",
        "                \n",
        "                # update critic model weights\n",
        "                c_loss = self.training_step_critic(X_real,X_fake, y_real,y_fake,half_batch)\n",
        "                avg_loss_critic(c_loss)\n",
        "                \n",
        "            gen_loss, matched_images, gen_images = self.training_step_generator(self.random_noise_size,class_type)\n",
        "            avg_loss_gen(gen_loss)\n",
        "            data_access.print_training_output(i,n_steps, avg_loss_critic.result(),avg_loss_gen.result()) \n",
        "            if((i % (n_steps / epoches)) == 0):\n",
        "                data_access.store_images_seed(directory,gen_images[:n_dif_images],epoch)\n",
        "                with sum_writer_loss.as_default():\n",
        "                    tf.summary.scalar('loss_gen', avg_loss_gen.result(),step=self.generator.optimizer.iterations)\n",
        "                    tf.summary.scalar('avg_loss_critic', avg_loss_critic.result(),step=self.critic.optimizer.iterations)\n",
        "                epoch += 1\n",
        "                if((epoch % 10) == 0):\n",
        "                    self.generator.save_weights('/content/weights/g_weights/g_weights',save_format='tf')\n",
        "                    self.critic.save_weights('/content/weights/c_weights/c_weights',save_format='tf')\n",
        "                    with open('current_epoch.txt','w') as ofil:\n",
        "                        ofil.write(f'{epoch}')\n",
        "                    print('Saved epoch ',epoch)\n",
        "                if((epoch % 20) == 0):\n",
        "                    if('weights.zip' in os.listdir('./drive/My Drive')):\n",
        "                        !rm '/content/drive/My Drive/weights.zip'\n",
        "                    !zip -r '/content/drive/My Drive/weights.zip' weights\n",
        "        data_access.create_collection(epoches,n_dif_images,directory)\n",
        "        data_access.print_training_time(start_time,time.time(),self.model_parameters)\n",
        "\n",
        "    def generate_images(self,number_of_samples,directory,class_names=None):\n",
        "        seed = tf.random.normal([number_of_samples, self.random_noise_size])\n",
        "        images = self.generator(seed,training=False)\n",
        "        if self.classifier is not None: \n",
        "            predictions = self.classifier(data_access.normalize(data_access.de_standardize(images)))\n",
        "            data_access.produce_generate_figure(directory,images,predictions,class_names)\n",
        "        else:\n",
        "            data_access.store_images_seed(directory,images,'gen_images','gan')\n",
        "   \n",
        "print('Tensorflow version: {}'.format(tf.__version__))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version: 2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvqb2NY3Zp-3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 200\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "class_names = ['1 waggon', '2 waggons', '3 waggons', '4 waggons']\n",
        "\n",
        "gan = WGAN_GP(classifier_filename='Number_Of_Wagons_0.9952_ConvExp14_t4000_v1000.hdf5')\n",
        "gan.load_dataset(data_access.prepare_data(generator='gan',size_shape=(152,152)),len(class_names))\n",
        "\n",
        "gan.train_model(EPOCHS)\n",
        "\n",
        "gan.generate_images(50,\"gen_images\", class_names)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewowbrnVZ7KZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    from google.colab import files\n",
        "except ImportError:\n",
        "    pass\n",
        "else:\n",
        "    !zip -r /content/gen_imgs.zip /content/gen_imgs\n",
        "    files.download(\"/content/gen_imgs.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7ASu9Q1Txv-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r imgs\n",
        "!rm -r weights\n",
        "!rm current_epoch.txt\n",
        "!rm -r logs"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}